# Kullback-Leibler Divergence and Inaugural Addresses

Kullback-Leibler divergence (KLD), also known as relative entropy, is a measure of the difference between two probability distributions. It is commonly used in information theory and machine learning to quantify the difference between a target distribution and an estimated distribution.  
In this study, KLD is used to diachronically measure the differences between US president's Inaugural Addresses. Due to natural language change, KLD is expected to increase over time. In a second step, on a more fine-grained level, the lexis and particular word contributions to KLD are analysed using word clouds.

